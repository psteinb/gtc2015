As shown in section \ref{sec:alg}, multi-view deconvolution relies on 3D image stack convolutions with large kernels. In order to iterate towards an efficient design of multi-view deconvolution, this section will discuss model implementations and their benchmarks on hardware introduced in section \ref{sec:hw}. All of the model systems use synthetic data which is validated after each run upon consistency where appropriate. If not stated otherwise, the synthetic data has been generated in steps of powers of 2 in each dimension, so that each benchmark was run on $64x64x64$, $128x64x64$, $128x128x64$, \dots, $1024x1024x1024$ shaped datasets if possible.

\subsection{Single FFT}

As the heart of the application are FFT, there performance on CPU and GPU should be compared. For this, the most simple setup is used.

\begin{lstlisting}[caption={Single FFT on synthetic data performed on CPU in pseudo-code based on the \fftw{} syntax.},label={lst:single_fft_cpu}]
stack_f32 synthetic_data;

fftwf_plan plan = fftw_plan_dft_r2c(3,
                                    synthetic_data.shape(),
                                    synthetic_data.ptr(),
                                    synthetic_data.ptr(),
                                    FFTW_MEASURE);
//start timing
fftwf_execute(plan)
//end timing
\end{lstlisting}

Listing \ref{lst:single_fft_cpu} gives an overview on the syntax required to execute a \fftw{} 3D FFT on spatial domain data and whose result is written back to the same memory location in the frequency domain (in-place transform). The application programming interface (API) requires plan to perform the transform operation. According to the documentation \cite{fftw_manual}, the plan allocates additional memory for structures required during FFT execution. This global variable hence exerts a software design force on all clients to be aware of this behavior. Moreover, plans should be reused in order to prevent unnecessary reallocations. The equivalent implementation to perform the FFT on the \gpu{} is given in listing \ref{lst:single_fft_gpu}.\newline

\begin{lstlisting}[caption={Single FFT on synthetic data performed on GPU in pseudo-code based on the \cufft{} syntax.},label={lst:single_fft_gpu}]
stack_f32 synthetic_data;
float* device_ptr = 0;
cudaMalloc(device_ptr, sizeof(float)*synthetic_data.size());
cudaMemcpy(device_ptr, synthetic_data.ptr() , sizeof(float)*synthetic_data.size(), cudaHostToDevice);

cufftHandle plan;
cufftPlan3d(&plan,synthetic_data.shape(0),synthetic_data.shape(1),synthetic_data.shape(2),CUFFT_R2C);

//start timing
cufftExecR2C(plan, device_ptr, device_ptr);
//end timing

cudaMemcpy(synthetic_data.ptr(), device_ptr , sizeof(float)*synthetic_data.size(), cudaDeviceToHost);
\end{lstlisting}

Clearly, the offload model of having \gpu{} memory being separate from \cpu{} memory requires a lot more syntax than for plain \cpu{} use cases. However, the \cufft{} API is similar in nature to \fftw{} except the more stringent procedural approach (no functions with return values, rather than functions operating on structs). As \texttt{cudaMemcpy} is used to transfer data from host to device and back, this implementation is internally synchronized.\newline

\begin{figure}[h]
  \centering
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/synced_gpu_runtime}
    \caption{CUDA runtime}
    \label{fig:single_fft_runtime}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/synced_gpu_api_fraction}
    \caption{CUDA API fraction}
    \label{fig:single_fft_api_fraction}
  \end{subfigure}
  \hfill
  \caption{Runtimes for single 3D FFT on synthetic data on \cpu{} with \fftw{} and on \gpu{} with \cufft{} (left, \ref{fig:single_fft_runtime}) and API time fraction of runtime dedicated to data (de-)allocation and/or data transfer (right, \ref{fig:single_fft_api_fraction}).}
  \label{fig:rt_single_fft}
\end{figure}

Figure \ref{fig:rt_single_fft} illustrates the runtime budget of an FFT transformation for different input data performed on multi-core \cpu{}s or on \gpu{}s. Subfigure \ref{fig:single_fft_runtime} shows a linear rise in runtime with increasing size of input data. This is the behaviour expected from considerations of equation \ref{eq:fft_convol_complexity}. Due to the larger memory available on \cpu{}s, they are able to handle bigger data sets than \gpu{}s.\\

For the execution on a \gpu{}, it is interesting to observe what fraction of this runtime is invested in data transfers. Figure \ref{fig:single_fft_api_fraction} hints to $\unit[50-70]{\%}$ of the total runtime is dedicated to data transfers. It is interesting to see how the \gpu{} that supports PCI Express Gen3 clearly dedicates less time to memory transfers than the other modesl. \\

The API fraction measurements have been performed with \texttt{nvprof} called as in:\newline
\begin{center}
  \texttt{nvprof --normalized-time-unit ms --print-gpu-summary --print-api-summary --print-summary --profile-from-start off}\newline
\end{center}

Firgure \ref{fig:single_fft_api_fraction} implies, that if data transfer can be hidden or accelerated, this would be beneficial for the \gpu{} based algorithm to accelerate in comparions to the \cpu{} implementation.

\clearpage
\subsection{Batched FFTs}
As the transform of a single data volume is of minimal use for the application at hand, the next step is to study the transform of a sequence of image stacks with fixed sizes. In all of the following figures and discussions, 8 image stacks were used per experiment and shape configuration. All given data volumes refer to the data processed during one iteration of the experiment (here, 10 repeats were used). \newline
 
\begin{lstlisting}[caption={Batched FFT on synthetic data performed on CPU in pseudo-code based on the \fftw{} syntax.},label={lst:batched_fft_cpu}]
stack_f32 synthetic_data[n_view];

fftwf_plan plan = fftw_plan_dft_r2c(3,
                                    synthetic_data[0].shape(),
                                    synthetic_data[0].ptr(),
                                    synthetic_data[0].ptr(),
                                    FFTW_MEASURE);
//start timing
for ( v : n_view )
  fftwf_execute_dft_r2c(plan, 
                        synthetic_data[v].ptr(),
                        synthetic_data[v].ptr());
//end timing
\end{lstlisting}

As listing \ref{lst:batched_fft_cpu} illustrates, plan reuse is performed and \fftw{} is called with the \texttt{FFTW\_MEASURE}. During plan creation, the \fftw{} library tries to find the best FFT implementation for the given buffer shape. \texttt{FFTW\_MEASURE} guarantees a next-to-optimal solution. The GPU implementation at this point already requires a complex orchestration of memory transfer streams and execution.

\begin{figure}[h]
  \centering

  \begin{subfigure}[b]{0.9\textwidth}
    \includegraphics[width=\textwidth]{img/bench_gpu_many_nd_fft_sync_narrow}
    \caption{synchronized transform}
    \label{fig:batched_fft_sync}
  \end{subfigure}%

  \begin{subfigure}[b]{0.9\textwidth}
    \includegraphics[width=\textwidth]{img/bench_gpu_many_nd_fft_async_narrow}
    \caption{asynchronous transform}
    \label{fig:batched_fft_async}
  \end{subfigure}%

  \begin{subfigure}[b]{0.9\textwidth}
    \includegraphics[width=\textwidth]{img/bench_gpu_many_nd_fft_async2plans_narrow}
    \caption{asynchronous transform with 2 streams}
    \label{fig:batched_fft_async2plans}
  \end{subfigure}%

  \caption{\texttt{nvprof} screenshot of batched FFT transforms.}
  \label{fig:nvprof_batched_fft}
\end{figure}

Figure \ref{fig:nvprof_batched_fft} illustrates possible advantages and disadvantages of the applied techniques. As the documentation of the corresponding code for figure \ref{fig:nvprof_batched_fft} would be rather long, the interested reader is deferred to \texttt{bench\_gpu\_many\_nd\_fft.cu} of the \lmvn{} benchmark suite available through \cite{lmvn_repo}. 

\begin{lstlisting}[caption={Asynchronous Batched FFT on synthetic data performed on GPU in pseudo-code based on the \cufft{} syntax.},label={lst:batched_fft_gpu_async2plans}]
stack_f32 synthetic_data[n_view];
cudaEvent_t stream_events[n_view];
fftwf_plan plans[2];
cudaStream_t streams[2];
float* d_buffer[2];
// ... initialize 

//start timing
for ( v : n_view ){
  cudaMemcpyAsync(synthetic_data[v] -> d_buffer[v \% 2],streams[v \% 2]);
  cudaEventRecord(stream_events[v],streams[v \% 2]);
  cudaStreamWaitEvent(streams[v \% 2],stream_events[v - 1]);
  cufftExecR2C(plans[v \% 2], d_buffer[v \% 2]) // on stream streams[v \% 2]
  cudaMemcpyAsync(d_buffer[v \% 2] -> synthetic_data[v],streams[v \% 2]);
}
cudaDeviceSynchronize();
//end timing

//clean-up
\end{lstlisting}


What however becomes clear is that synchronous (figure \ref{fig:batched_fft_sync}) are not beneficial as data transfers to the \gpu{} are performed sequentially. Asynchronous transfers using the same number of streams as there are image stacks (figure \ref{fig:batched_fft_async}) are also not performant (mostly due to the lack of copy engines). But if the number of transfer streams correlates with the number of copy engines the \gpu{} offers, data transfers and computations can be overlaid to some higher extent and runtime benefits can be obtained (figure \ref{fig:batched_fft_async2plans}). Besides the ones just discussed, other host-device data transfer methods were benchmarked. 
For example, mapped memory usage (buffers are marked by \texttt{cudaHostRegister} and their device pointer is obtained through \texttt{cudaHostGetDevicePointer}) and managed unified memory (all data is allocated by \texttt{cudaMallocManaged}) have been implemented. The results for ``managed'' and ``mapped'' memory usage are not shown here because time measurements did not correlated at all with the measurements obtained in \texttt{nvprof}. Before drawing wrong conclusions from an illposed measurement, the data is not discussed. 

\begin{figure}[h]
  \centering
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/batched_cgpu_runtime}
    \caption{CUDA runtime}
    \label{fig:batched_fft_runtime}
  \end{subfigure}%
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{plots/batched_gpu_speed_up}
    \caption{\gpu{} speed-up versus \cpu{}}
    \label{fig:batched_fft_speed_up}
  \end{subfigure}
  \hfill
  \caption{Runtimes for 3D FFT transforms on a batch of 8 buffers from synthetic data. The results are compared to the \cpu{} implementation with \fftw{} and on the \gpu{} with \cufft{} (left, \ref{fig:single_fft_runtime}) and the relative speed-up of the \gpu{} implementation (see figure \ref{fig:batched_fft_async2plans}) compared to the multi-core \cpu{} implementation (right, \ref{fig:batched_fft_speed_up}) for different \gpu{} models.}
  \label{fig:rt_batched_fft}
\end{figure}


The synchronized implementation in figure \ref{fig:rt_batched_fft} is on par with the \cpu{} implementation for a wide range of data volumes. The differences between results from the ``K20c'' and the ``Titan Black'' \gpu{} re-confirms earlier findings from figure \ref{fig:single_fft_api_fraction}. The implementation with \texttt{cufftPlanMany} is also show and clearly breaks down with smaller image stack sizes due to \gpu{} memory constraints. The reason for this lies in the fact that \texttt{cufftPlanMany} requires all input data to be present on the \gpu{}.\\
 
The implementation referred to as ``async2plans'' in figure \ref{fig:rt_batched_fft} notes the results of listing \ref{lst:batched_fft_gpu_async2plans} in practise. Given an input data volume of $\unit[50-250]{MB}$, this implementation is faster than the \cpu{} implementation, but does not attain results above a certain data volume compared to e.g. the ``sync'' method. This is due to the fact that it requires device memory of the size of 2 input stacks as well as the workarea for two FFT transforms. This of course depends on the \gpu{} model as to what amount of device memory is available. % This is due to the fact that both the GeForce Titan Black and the Tesla M2090 do only contain 1 copy engine. The device is henceforth forced to serialize asynchronous data transfers and kernel execution.
\newline

For large data sets, the real-to-complex FFT transform performed on a \gpu{} appears not to supersede the performance of \cpu{} based implementations by a large amount. As for larger memory transfers, the time per transfer increases dramatically over the time needed for FFT relevant computations, this effect can be attributed to the PCI Express bus bandwidth and speed. Thus, the performance of ``sync'' and ``planmany'' implementations in figure \ref{fig:rt_batched_fft} must exhibit a similar behavior.\newline



\clearpage
\subsection{Batched Convolutions}

The next level of discussing the performance of multi-view deconvolution is to look at batched 3D image convolutions. Synthetic data was used again to make up 8 training data sets, all of which were convolve with a filter kernel of size $21 \times 21 \times 21$. In order to remain as close as possible to the final use case, 8 synthetic kernel image stacks are generated accordingly. All given data volumes below refer to the data processed during one iteration of the experiment (here, 10 repeats were used). \newline

\begin{lstlisting}[caption={Asynchronous batched FFT based convolution on synthetic data performed with CUDA. The algorithm is given in pseudo-code based on the CUDA and \cufft{} API.},label={lst:batched_folds}]
stack_f32 synthetic_data[n_view];
stack_f32 kernel_data[n_view];
fftwf_plan plans[2];
cudaStream_t streams[2];
float* d_buffer[2];
// ... initialize 

inplace_forward_transform(kernel_data[:]);

//start timing
cudaMemcpyAsync(kernel_data[0] -> d_buffer[kernel],streams[kernel]);
  
for ( v : n_view ){
  cudaMemcpyAsync(synthetic_data[v] -> d_buffer[image],streams[image]);
  cufftExecR2C(plans[image], d_buffer[image]) // on stream streams[image]
  
  cudaDeviceSynchronize();

  multiply<<< .. , .. , 0 , streams[image]>>>(d_buffer[image],d_buffer[kernel],...);
  cudaMemcpyAsync(kernel_data[v+1] -> d_buffer[kernel],streams[kernel]);
  
  cufftExecC2R(plans[image], d_buffer[image]) // on stream streams[image]
  cudaMemcpyAsync(d_buffer[image] -> synthetic_data[v],streams[image]);
}
cudaDeviceSynchronize();
//end timing

//clean-up
\end{lstlisting}

Listing \ref{lst:batched_folds} describes the implementation chosen. Contrary to the discussion of the previous section, it is unfeasible to use event driven synchronisation at this point. The reason for this is, that the multiplication step required by the convolution theorem (equation \ref{eq:convol_theorem}) imposes an implicit barrier to the algorithm because both the forward transformed image and filter kernel have to be present in order for the multiplication to take place. The function denoted \texttt{inplace\_forward\_transform} will perform the real-to-complex tranforms of all filter kernels before the actual loop through the image stacks. If the implementation would not execute this transform before the loop, the FFT transform of the kernel filters would need to be performed inside the main loop. This however would inly be parallisable on Tesla \gpu{}s because they hold 2 copy engines that allow 2 parallel to-device and to-host transfers.  

\begin{figure}[hbt]
  \centering
    \includegraphics[width=\textwidth]{plots/batched_folds_gpu_speed_up}
  \caption{Speed-Ups for 3D FFT based image convolutions on a batch of 8 image buffers from synthetic data and 8 kernel filters. The results are compared to the \cpu{} implementation with \fftw{} for different \gpu{} models.}
  \label{fig:su_batched_fold}
\end{figure}

Figure \ref{fig:su_batched_fold} reports the speed-up measurements where the baseline \cpu{} implementation was obtained with \fftw{} on a 16 core Haswell host. The generel trend is again similar to observations from figure \ref{fig:rt_batched_fft}. This is due to the fact that, the additional computational expenses from the multiplication step do not mitigate the overall asymmetry of transfer timing versus computational effots. This is most visible for the ``plan\_many'' implementation, which suffers greatly from this effect. Further, the ``interleaved'' algorithm from listing \ref{lst:batched_folds} is found to have superiour performance to the ``plan\_many'' implementation and also is capable of consuming larger datasets, which is the final goal with respect to \lmvn{}. 
 
\clearpage
\subsection{Library Implementation}

For implementing \lmvn{} according to listing \ref{lst:java_implementation}, two strategies will be compared. First, the algorithm could be implemented entirely on device, i.e. all input and output data (if possible) is copied to the \gpu{} where actual iterative deconvolution is performed. As a second alternative, an interleaved strategy is chosed again as it has provided promising results in the previous discussion. All given data volumes below refer to the data processed during one iteration of the experiment (here, 10 repeats were used).  \newline

\begin{lstlisting}[caption={Interleaved implementation of Multi-View Deconvolution. The algorithm is given in pseudo-code based on the CUDA and \cufft{} API.},label={lst:interleaved_mvd}]
stack_f32 psi = stack_f32(const);
stack_f32 view[n_view], kernel1[n_view],     //loaded from disk
	  kernel2[n_view], weights[n_view];  //loaded from disk

float* d_buffers[4];
cudaStream_t streams[2];
//initialize

inplace_forward_transform(kernel1[:]);
inplace_forward_transform(kernel2[:]);

//end timing
for( i : n_iterations ){
  cudaMemcpyAsync(kernel1[0] -> d_buffers[0],streams[0]);  
  for( v : n_view ){
    cudaMemcpy(d_buffers[psi] -> d_buffers[temp]); 
    d_buffers[temp] = convolve(d_buffers[temp],d_buffers[0],stream[0]);
    cudaMemcpyAsync(kernel2[v] -> d_buffers[0],streams[0]);
    cudaMemcpyAsync(view[v] -> d_buffers[1],streams[1]);
    divide<<<..,..,0,streams[1]>>>(d_buffers[1],d_buffers[temp]);
    d_buffers[temp] = convolve(d_buffers[temp],d_buffers[0],stream[0]);
    cudaMemcpyAsync(weights[v] -> d_buffers[1],streams[1]);
    psi = regularize(d_buffers[psi], d_buffers[temp], d_buffers[0]);
    HANDLE_ERROR(cudaDeviceSynchronize());
  }
}
//end timing

//clean-up
\end{lstlisting}

Listing \ref{lst:interleaved_mvd} tries to summarize the basic structure of the interleaved implementation of the multi-view deconvolution algorithm chosen. Many initialization, cross checking and cleaning-up steps are left out for clearity. Compute steps are overlaid with host-device transfers to a high degree. Currently, the synchronisation is done through \texttt{cudaDeviceSynchronize}, although that an event-based synchronisation might provide more fine granularity. This optimisation is left out for later studies.\newline

\begin{figure}[hbt]
  \centering
    \includegraphics[width=\textwidth]{plots/deconvolve_synthetic_gpu_speed_up}
  \caption{Speed-Ups for 3D FFT based image convolutions on a batch of 8 image buffers from synthetic data and 8 kernel filters. The results are compared to the \cpu{} implementation with \fftw{} for different \gpu{} models.}
  \label{fig:synthetic_deconvolution}
\end{figure}

Figure \ref{fig:synthetic_deconvolution} summarizes the performance gain through the interleaving strategy. We can reproduce the speed-up of $2$ that \cite{2013arXiv1308.0730P} described as well. The different numbers for the ``all\_on\_device'' implementation on the \textit{Tesla K20c} versus the \textit{GeForce Titan Black} can be attributed to the higher clock rate and overall higher computational power of the latter. The ``all\_on\_device'' curve ends at $\unit[32*4*6 = 768]{MB}$ because the experiment requires not only additional temporary buffers (``temp'', ``psi''), but also the workarea for the two involve FFT operations. The performance increase towards large data sets is due to the more favorible ration of computations versus memory transfers here.\newline

By the nature of the interleaved strategy, it is capable of handling far more data than the standard ``all\_on\_device'' version. It programmatically uploads data from the \gpu{} host when and if it is needed. Further, strict concurrent execution of computation and memory transfers accelerate this method even more which enables it to deliver up to 7 times the performance of a $2\times{}8$-core Haswell \cpu{}. The performance difference between the \textit{Tesla K20c} and the \textit{GeForce Titan Black} shows itself in a modest fashion, as listing \ref{lst:interleaved_mvd} contains less parallel transfer regions than the model systems discussed above.


